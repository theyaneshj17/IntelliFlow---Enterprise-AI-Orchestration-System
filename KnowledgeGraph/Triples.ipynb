{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbec6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Extracting text from PDF...\n",
      "‚úÇÔ∏è Chunking text...\n",
      "Created 49 chunks\n",
      "üß† Starting triple extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [04:50<00:00,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving results...\n",
      "‚úÖ Saved 1060 triples to research_paper_triples.csv\n",
      "‚úÖ Metrics saved to extraction_metrics.csv\n",
      "‚úÖ Neo4j-ready triples saved to neo4j_triples.txt\n",
      "\n",
      "üìä SUMMARY:\n",
      "Total triples extracted: 1060\n",
      "Average triples per chunk: 21.63\n",
      "\n",
      "üîç SAMPLE TRIPLES:\n",
      "1. (Ashish Vaswani) --[is affiliated with]--> (Google Brain)\n",
      "2. (Noam Shazeer) --[is affiliated with]--> (Google Brain)\n",
      "3. (Niki Parmar) --[is affiliated with]--> (Google Research)\n",
      "4. (Jakob Uszkoreit) --[is affiliated with]--> (Google Research)\n",
      "5. (Llion Jones) --[is affiliated with]--> (Google Research)\n",
      "6. (Aidan N. Gomez) --[is affiliated with]--> (University of Toronto)\n",
      "7. (≈Åukasz Kaiser) --[is affiliated with]--> (Google Brain)\n",
      "8. (Illia Polosukhin) --[is affiliated with]--> (Google Research)\n",
      "9. (Transformer) --[is a]--> (new simple network architecture)\n",
      "10. (Transformer) --[is based solely on]--> (attention mechanisms)\n",
      "\n",
      "‚úÖ DONE! Check the output files in your Downloads folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import anthropic\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1Ô∏è‚É£ Claude API client\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=\"*****\"\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ PDF Processing Function\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF file\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        full_text = \"\"\n",
    "        \n",
    "        for page_num, page in enumerate(pdf_reader.pages):\n",
    "            page_text = page.extract_text()\n",
    "            full_text += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\"\n",
    "    \n",
    "    return full_text\n",
    "\n",
    "# 3Ô∏è‚É£ Text Chunking Function  \n",
    "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \";\", \":\", \",\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Create DataFrame with chunk information\n",
    "    chunk_data = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_data.append({\n",
    "            \"doc_ID\": \"research_paper\",\n",
    "            \"chunk_ID\": f\"chunk_{i+1:03d}\",\n",
    "            \"chunk\": chunk,\n",
    "            \"chunk_length\": len(chunk)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(chunk_data)\n",
    "\n",
    "# 4Ô∏è‚É£ Updated prompt for research papers\n",
    "def build_primary_prompt(text_chunk):\n",
    "    return f\"\"\"\n",
    "You are a research paper knowledge extraction assistant specialized in comprehensive triple extraction. Your task is to extract ALL relevant information from the provided research paper CHUNK.\n",
    "\n",
    "1. Named Entities:\n",
    "Extract all distinct entities mentioned in the text, including:\n",
    "- Authors and researchers\n",
    "- Institutions and organizations\n",
    "- Research methodologies and techniques\n",
    "- Algorithms and models\n",
    "- Datasets and data sources\n",
    "- Software tools and frameworks\n",
    "- Technical concepts and terms\n",
    "- Performance metrics and measurements\n",
    "- Experimental results and findings\n",
    "- Related work and citations\n",
    "- Mathematical concepts and formulas\n",
    "- System architectures and components\n",
    "- Evaluation criteria and benchmarks\n",
    "- Statistical measures (accuracy, precision, recall, F1-score, p-values, etc.)\n",
    "- Research contributions and innovations\n",
    "- Limitations and challenges\n",
    "- Future work directions\n",
    "- Any other domain-specific entities\n",
    "\n",
    "2. Triples:\n",
    "Extract ALL subject‚Äìpredicate‚Äìobject relationships between those entities.\n",
    "BE EXHAUSTIVE - don't miss any relationships, even subtle ones.\n",
    "\n",
    "---\n",
    "## Special Handling Rules:\n",
    "- **Coreference**:\n",
    "    - Resolve pronouns or references to the correct entity.\n",
    "    - Example:  \n",
    "      Text: \"The algorithm improves accuracy. This method outperforms baselines.\"  \n",
    "      ‚Üí Triples:\n",
    "        - (\"The algorithm\", \"improves\", \"accuracy\")\n",
    "        - (\"The algorithm\", \"outperforms\", \"baselines\")\n",
    "\n",
    "- **Apposition**:\n",
    "    - Create an \"is\" triple for appositive structures.\n",
    "    - Example:  \n",
    "      Text: \"BERT, a transformer model, achieves state-of-the-art results.\"  \n",
    "      ‚Üí Triples:\n",
    "        - (\"BERT\", \"is\", \"transformer model\")\n",
    "        - (\"BERT\", \"achieves\", \"state-of-the-art results\")\n",
    "\n",
    "- **Multiple Subjects**:\n",
    "    - Split coordinated subjects into individual triples.\n",
    "    - Example:  \n",
    "      Text: \"CNN and RNN models were evaluated on the dataset.\"  \n",
    "      ‚Üí Triples:\n",
    "        - (\"CNN\", \"was evaluated on\", \"dataset\")\n",
    "        - (\"RNN\", \"was evaluated on\", \"dataset\")\n",
    "\n",
    "- **Implicit Relationships**:\n",
    "    - Extract relationships that are implied but not directly stated.\n",
    "    - Example:\n",
    "      Text: \"We use PyTorch to implement the neural network which processes image data.\"\n",
    "      ‚Üí Triples:\n",
    "        - (\"Authors\", \"use\", \"PyTorch\")\n",
    "        - (\"PyTorch\", \"implements\", \"neural network\")\n",
    "        - (\"Neural network\", \"processes\", \"image data\")\n",
    "        - (\"Authors\", \"implement\", \"neural network\")\n",
    "\n",
    "- **Negations and Qualifiers**:\n",
    "    - Preserve negations in the predicate.\n",
    "    - Example:\n",
    "      Text: \"The model does not require labeled data.\"\n",
    "      ‚Üí Triple:\n",
    "        - (\"Model\", \"does not require\", \"labeled data\")\n",
    "        \n",
    "- **Performance and Results**:\n",
    "    - Extract precise performance relationships and metrics.\n",
    "    - Example:\n",
    "      Text: \"Our method achieved 95% accuracy on MNIST dataset, outperforming the baseline by 5%.\"\n",
    "      ‚Üí Triples:\n",
    "        - (\"Our method\", \"achieved\", \"95% accuracy\")\n",
    "        - (\"Our method\", \"evaluated on\", \"MNIST dataset\")\n",
    "        - (\"Our method\", \"outperforms\", \"baseline\")\n",
    "        - (\"Our method\", \"outperforms baseline by\", \"5%\")\n",
    "\n",
    "- **Research Relationships**:\n",
    "    - Extract relationships about research process, contributions, and comparisons.\n",
    "    - Example:\n",
    "      Text: \"Smith et al. (2020) proposed a similar approach, but our work extends it by adding attention mechanisms.\"\n",
    "      ‚Üí Triples:\n",
    "        - (\"Smith et al. (2020)\", \"proposed\", \"similar approach\")\n",
    "        - (\"Our work\", \"extends\", \"Smith et al. approach\")\n",
    "        - (\"Our work\", \"adds\", \"attention mechanisms\")\n",
    "\n",
    "---\n",
    "\n",
    "## Output Format:\n",
    "#### ENTITIES:\n",
    "- Entity 1\n",
    "- Entity 2\n",
    "...\n",
    "\n",
    "#### TRIPLES:\n",
    "(\"Subject\", \"Predicate\", \"Object\")\n",
    "(\"Subject\", \"Predicate\", \"Object\")\n",
    "...\n",
    "\n",
    "---\n",
    "\n",
    "TEXT:\n",
    "{text_chunk}\n",
    "\n",
    "#### ENTITIES:\n",
    "\"\"\"\n",
    "\n",
    "# 5Ô∏è‚É£ Updated verification prompt for research papers\n",
    "def build_verification_prompt(text_chunk, entities, triples):\n",
    "    return f\"\"\"\n",
    "You are an expert research paper knowledge extraction verification system. Review this text and the already extracted entities and triples to identify ANY MISSED relationships.\n",
    "\n",
    "TEXT:\n",
    "{text_chunk}\n",
    "\n",
    "ALREADY EXTRACTED ENTITIES:\n",
    "{entities}\n",
    "\n",
    "ALREADY EXTRACTED TRIPLES:\n",
    "{triples}\n",
    "\n",
    "YOUR TASK:\n",
    "1. Carefully analyze if ANY possible relationship between entities was missed\n",
    "2. Look specifically for:\n",
    "   - Author-contribution relationships\n",
    "   - Method-performance relationships\n",
    "   - Dataset-evaluation relationships\n",
    "   - Comparison relationships between methods/models\n",
    "   - Temporal relationships (before/after, previous work)\n",
    "   - Causal relationships (causes, enables, improves)\n",
    "   - Implementation relationships (uses, implements, builds upon)\n",
    "   - Experimental relationships (tested on, evaluated with, compared against)\n",
    "   - Citation relationships (referenced by, builds on, extends)\n",
    "   - Technical relationships (component of, part of, consists of)\n",
    "   - Performance relationships (achieves, scores, measures)\n",
    "   - Research contribution relationships (proposes, introduces, develops)\n",
    "\n",
    "Output ONLY additional triples that were missed. Format as:\n",
    "#### ADDITIONAL TRIPLES:\n",
    "(\"Subject\", \"Predicate\", \"Object\")\n",
    "(\"Subject\", \"Predicate\", \"Object\")\n",
    "...\n",
    "\n",
    "If no additional triples can be found, respond with \"No additional triples identified.\"\n",
    "\"\"\"\n",
    "\n",
    "# 6Ô∏è‚É£ Claude call with retry logic (unchanged)\n",
    "def query_claude(prompt, model=\"claude-3-haiku-20240307\", retries=2, backoff=3):\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            response = client.messages.create(\n",
    "                model=model,\n",
    "                max_tokens=4000,\n",
    "                temperature=0.0,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.content[0].text.strip()\n",
    "        except Exception as e:\n",
    "            if attempt < retries:\n",
    "                print(f\"Retry {attempt+1}/{retries} after error: {str(e)}\")\n",
    "                time.sleep(backoff * (attempt + 1))\n",
    "            else:\n",
    "                return str(e)\n",
    "\n",
    "# 7Ô∏è‚É£ Parse functions (unchanged)\n",
    "def parse_claude_output(output):\n",
    "    entities = []\n",
    "    triples = []\n",
    "    \n",
    "    entities_section = re.search(r'#### ENTITIES:(.*?)(?=####|$)', output, re.DOTALL)\n",
    "    if entities_section:\n",
    "        entities_text = entities_section.group(1).strip()\n",
    "        entities = [entity.strip('- ').strip() for entity in entities_text.split('\\n') if entity.strip('- ').strip()]\n",
    "    \n",
    "    triple_pattern = r'\\(\"([^\"]+)\", \"([^\"]+)\", \"([^\"]+)\"\\)'\n",
    "    for match in re.findall(triple_pattern, output):\n",
    "        subject, predicate, obj = match\n",
    "        triples.append((subject.strip(), predicate.strip(), obj.strip()))\n",
    "    \n",
    "    return entities, triples\n",
    "\n",
    "def parse_verification_output(output):\n",
    "    additional_triples = []\n",
    "    triple_pattern = r'\\(\"([^\"]+)\", \"([^\"]+)\", \"([^\"]+)\"\\)'\n",
    "    for match in re.findall(triple_pattern, output):\n",
    "        subject, predicate, obj = match\n",
    "        additional_triples.append((subject.strip(), predicate.strip(), obj.strip()))\n",
    "    return additional_triples\n",
    "\n",
    "# üî• MAIN PROCESSING FUNCTION\n",
    "def process_research_paper(pdf_path, output_dir=\"C:/Users/theya/Downloads/\"):\n",
    "    \"\"\"\n",
    "    Main function to process research paper PDF and extract triples\n",
    "    \"\"\"\n",
    "    print(\"üìÑ Extracting text from PDF...\")\n",
    "    full_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"‚úÇÔ∏è Chunking text...\")\n",
    "    df = chunk_text(full_text, chunk_size=1000, chunk_overlap=200)\n",
    "    print(f\"Created {len(df)} chunks\")\n",
    "    \n",
    "    # Optional: TEST with first few chunks only\n",
    "    # df = df.head(3)  # Uncomment for testing\n",
    "    \n",
    "    print(\"üß† Starting triple extraction...\")\n",
    "    \n",
    "    # Process chunks\n",
    "    structured_results = []\n",
    "    error_logs = []\n",
    "    coverage_metrics = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing chunks\"):\n",
    "        doc_id = row[\"doc_ID\"]\n",
    "        chunk_id = row[\"chunk_ID\"]\n",
    "        text_chunk = str(row[\"chunk\"])\n",
    "        chunk_triples = []\n",
    "        \n",
    "        # First pass - extract entities and triples\n",
    "        primary_prompt = build_primary_prompt(text_chunk)\n",
    "        primary_output = query_claude(primary_prompt)\n",
    "        \n",
    "        if \"error\" in primary_output.lower() or \"RateLimit\" in primary_output:\n",
    "            error_logs.append({\n",
    "                \"doc_id\": str(doc_id),\n",
    "                \"chunk_id\": str(chunk_id),\n",
    "                \"error_message\": primary_output,\n",
    "                \"phase\": \"primary_extraction\"\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        entities, primary_triples = parse_claude_output(primary_output)\n",
    "        \n",
    "        # Calculate initial metrics\n",
    "        entity_count = len(entities)\n",
    "        primary_triple_count = len(primary_triples)\n",
    "        \n",
    "        # Format triples for verification\n",
    "        formatted_triples = \"\\n\".join([f'(\"{s}\", \"{p}\", \"{o}\")' for s, p, o in primary_triples])\n",
    "        formatted_entities = \"- \" + \"\\n- \".join(entities)\n",
    "        \n",
    "        # Second pass - verification for missed relationships\n",
    "        verification_prompt = build_verification_prompt(text_chunk, formatted_entities, formatted_triples)\n",
    "        verification_output = query_claude(verification_prompt)\n",
    "        \n",
    "        additional_triples = []\n",
    "        if \"no additional triples\" not in verification_output.lower():\n",
    "            additional_triples = parse_verification_output(verification_output)\n",
    "        \n",
    "        # Combine and deduplicate triples\n",
    "        all_triples = primary_triples + additional_triples\n",
    "        unique_triples = []\n",
    "        seen = set()\n",
    "        \n",
    "        for s, p, o in all_triples:\n",
    "            triple_key = (s.lower(), p.lower(), o.lower())\n",
    "            if triple_key not in seen:\n",
    "                seen.add(triple_key)\n",
    "                unique_triples.append((s, p, o))\n",
    "                \n",
    "                # Add to structured results\n",
    "                chunk_triples.append({\n",
    "                    \"doc_id\": str(doc_id),\n",
    "                    \"chunk_id\": str(chunk_id),\n",
    "                    \"Subject\": s,\n",
    "                    \"Predicate\": p,\n",
    "                    \"Object\": o,\n",
    "                    \"source\": \"primary\" if (s, p, o) in primary_triples else \"verification\"\n",
    "                })\n",
    "        \n",
    "        # Calculate coverage metrics\n",
    "        metrics = {\n",
    "            \"doc_id\": str(doc_id),\n",
    "            \"chunk_id\": str(chunk_id),\n",
    "            \"text_length\": len(text_chunk),\n",
    "            \"entity_count\": entity_count,\n",
    "            \"primary_triple_count\": primary_triple_count,\n",
    "            \"additional_triple_count\": len(additional_triples),\n",
    "            \"final_triple_count\": len(unique_triples),\n",
    "            \"triple_entity_ratio\": len(unique_triples) / max(entity_count, 1),\n",
    "            \"triples_per_100_chars\": (len(unique_triples) * 100) / max(len(text_chunk), 1)\n",
    "        }\n",
    "        coverage_metrics.append(metrics)\n",
    "        \n",
    "        # Add to results\n",
    "        structured_results.extend(chunk_triples)\n",
    "        \n",
    "        time.sleep(1.2)  # Rate limiting\n",
    "    \n",
    "    # Save results\n",
    "    print(\"üíæ Saving results...\")\n",
    "    \n",
    "    # Save triples\n",
    "    output_df = pd.DataFrame(structured_results)\n",
    "    output_df.to_csv(f\"{output_dir}research_paper_triples.csv\", index=False)\n",
    "    print(f\"‚úÖ Saved {len(structured_results)} triples to research_paper_triples.csv\")\n",
    "    \n",
    "    # Save error log\n",
    "    if error_logs:\n",
    "        error_df = pd.DataFrame(error_logs)\n",
    "        error_df.to_csv(f\"{output_dir}extraction_errors.csv\", index=False)\n",
    "        print(f\"‚ö†Ô∏è {len(error_logs)} errors saved to extraction_errors.csv\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_df = pd.DataFrame(coverage_metrics)\n",
    "    metrics_df.to_csv(f\"{output_dir}extraction_metrics.csv\", index=False)\n",
    "    print(\"‚úÖ Metrics saved to extraction_metrics.csv\")\n",
    "    \n",
    "    # Save formatted triples for Neo4j import\n",
    "    with open(f\"{output_dir}neo4j_triples.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in structured_results:\n",
    "            # Format: (subject)-[predicate]->(object)\n",
    "            triple = f'(\"{row[\"Subject\"]}\", \"{row[\"Predicate\"]}\", \"{row[\"Object\"]}\")'\n",
    "            f.write(triple + \"\\n\")\n",
    "    print(\"‚úÖ Neo4j-ready triples saved to neo4j_triples.txt\")\n",
    "    \n",
    "    return structured_results, coverage_metrics\n",
    "\n",
    "# üöÄ USAGE EXAMPLE\n",
    "if __name__ == \"__main__\":\n",
    "    # UPDATE THIS PATH TO YOUR PDF\n",
    "    pdf_path = \"Paper.pdf\"\n",
    "    \n",
    "    # Process the paper\n",
    "    triples, metrics = process_research_paper(pdf_path)\n",
    "    \n",
    "    print(f\"\\nüìä SUMMARY:\")\n",
    "    print(f\"Total triples extracted: {len(triples)}\")\n",
    "    print(f\"Average triples per chunk: {len(triples)/len(metrics):.2f}\")\n",
    "    \n",
    "    # Show sample triples\n",
    "    print(f\"\\nüîç SAMPLE TRIPLES:\")\n",
    "    for i, triple in enumerate(triples[:10]):\n",
    "        print(f\"{i+1}. ({triple['Subject']}) --[{triple['Predicate']}]--> ({triple['Object']})\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ DONE! Check the output files in your Downloads folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb53ee11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b9ca8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç VERIFYING CHUNK COVERAGE...\n",
      "üìÑ ORIGINAL PDF:\n",
      "   Total pages: 15\n",
      "   Total characters: 39,718\n",
      "   Word count (approx): 6,058\n",
      "\n",
      "‚úÇÔ∏è CHUNKING RESULTS:\n",
      "   Total chunks created: 49\n",
      "   Your processing result: 49 chunks\n",
      "   ‚úÖ Match: YES\n",
      "\n",
      "üìè CHUNK SIZE ANALYSIS:\n",
      "   Average chunk length: 960 chars\n",
      "   Minimum chunk length: 836 chars\n",
      "   Maximum chunk length: 999 chars\n",
      "   Total chars in chunks: 47,048\n",
      "   Coverage: 118.5%\n",
      "\n",
      "üéØ CONTENT COVERAGE CHECK:\n",
      "   Estimated coverage: 94.3%\n",
      "   ‚úÖ GOOD: Good coverage, acceptable overlap\n",
      "\n",
      "üìä EXTRACTION QUALITY:\n",
      "   Chunks processed: 49\n",
      "   Total triples: 1060\n",
      "   Avg triples/chunk: 21.6\n",
      "   Low-quality chunks (<5 triples): 2\n",
      "   ‚ö†Ô∏è Check these chunks: ['chunk_048', 'chunk_049']\n",
      "\n",
      "üìà TRIPLE DISTRIBUTION:\n",
      "   Min triples per chunk: 0\n",
      "   Max triples per chunk: 87\n",
      "   Median triples per chunk: 17.0\n",
      "\n",
      "üîç SAMPLE CHUNKS:\n",
      "   Chunk 1: 996 chars\n",
      "   Preview: --- Page 1 ---\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce...\n",
      "\n",
      "   Chunk 25: 954 chars\n",
      "   Preview: heads clearly learn to perform different tasks, many appear to exhibit behavior related to the synta...\n",
      "\n",
      "   Chunk 49: 836 chars\n",
      "   Preview: 14\n",
      "--- Page 15 ---\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "...\n",
      "\n",
      "============================================================\n",
      "‚ö†Ô∏è VERDICT: Consider adjusting chunk settings\n",
      "   - Potential content loss\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import PyPDF2\n",
    "\n",
    "def verify_chunk_coverage(pdf_path=\"Paper.pdf\", \n",
    "                         output_dir=\"C:/Users/theya/Downloads/\"):\n",
    "    \"\"\"\n",
    "    Verify if your 49 chunks captured all content properly\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîç VERIFYING CHUNK COVERAGE...\")\n",
    "    \n",
    "    # 1. Extract original PDF text\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        full_text = \"\"\n",
    "        \n",
    "        for page_num, page in enumerate(pdf_reader.pages):\n",
    "            page_text = page.extract_text()\n",
    "            full_text += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\"\n",
    "    \n",
    "    # 2. Recreate the chunking process\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,        # Your current setting\n",
    "        chunk_overlap=200,      # Your current setting\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \";\", \":\", \",\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    \n",
    "    # 3. Basic coverage verification\n",
    "    print(f\"üìÑ ORIGINAL PDF:\")\n",
    "    print(f\"   Total pages: {len(pdf_reader.pages)}\")\n",
    "    print(f\"   Total characters: {len(full_text):,}\")\n",
    "    print(f\"   Word count (approx): {len(full_text.split()):,}\")\n",
    "    \n",
    "    print(f\"\\n‚úÇÔ∏è CHUNKING RESULTS:\")\n",
    "    print(f\"   Total chunks created: {len(chunks)}\")\n",
    "    print(f\"   Your processing result: 49 chunks\")\n",
    "    print(f\"   ‚úÖ Match: {'YES' if len(chunks) == 49 else 'NO - MISMATCH!'}\")\n",
    "    \n",
    "    # 4. Chunk size analysis\n",
    "    chunk_lengths = [len(chunk) for chunk in chunks]\n",
    "    \n",
    "    print(f\"\\nüìè CHUNK SIZE ANALYSIS:\")\n",
    "    print(f\"   Average chunk length: {sum(chunk_lengths)/len(chunk_lengths):.0f} chars\")\n",
    "    print(f\"   Minimum chunk length: {min(chunk_lengths)} chars\")\n",
    "    print(f\"   Maximum chunk length: {max(chunk_lengths)} chars\")\n",
    "    print(f\"   Total chars in chunks: {sum(chunk_lengths):,}\")\n",
    "    print(f\"   Coverage: {(sum(chunk_lengths)/len(full_text)*100):.1f}%\")\n",
    "    \n",
    "    # 5. Check for potential content loss\n",
    "    # Reconstruct text from chunks (without overlap)\n",
    "    reconstructed_length = 0\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i == 0:\n",
    "            reconstructed_length += len(chunk)\n",
    "        else:\n",
    "            # Remove overlap (approximate)\n",
    "            reconstructed_length += len(chunk) - 200  # overlap size\n",
    "    \n",
    "    coverage_ratio = reconstructed_length / len(full_text)\n",
    "    \n",
    "    print(f\"\\nüéØ CONTENT COVERAGE CHECK:\")\n",
    "    print(f\"   Estimated coverage: {coverage_ratio*100:.1f}%\")\n",
    "    if coverage_ratio > 0.95:\n",
    "        print(\"   ‚úÖ EXCELLENT: Very high coverage, minimal content loss\")\n",
    "    elif coverage_ratio > 0.85:\n",
    "        print(\"   ‚úÖ GOOD: Good coverage, acceptable overlap\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è WARNING: Potential content loss detected\")\n",
    "    \n",
    "    # 6. Load your extraction results for quality check\n",
    "    try:\n",
    "        triples_df = pd.read_csv(f\"{output_dir}research_paper_triples.csv\")\n",
    "        metrics_df = pd.read_csv(f\"{output_dir}extraction_metrics.csv\")\n",
    "        \n",
    "        print(f\"\\nüìä EXTRACTION QUALITY:\")\n",
    "        print(f\"   Chunks processed: {len(metrics_df)}\")\n",
    "        print(f\"   Total triples: {len(triples_df)}\")\n",
    "        print(f\"   Avg triples/chunk: {len(triples_df)/len(metrics_df):.1f}\")\n",
    "        \n",
    "        # Check for empty or low-quality chunks\n",
    "        low_quality_chunks = metrics_df[metrics_df['final_triple_count'] < 5]\n",
    "        print(f\"   Low-quality chunks (<5 triples): {len(low_quality_chunks)}\")\n",
    "        \n",
    "        if len(low_quality_chunks) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è Check these chunks: {low_quality_chunks['chunk_id'].tolist()}\")\n",
    "        \n",
    "        # Show chunk distribution\n",
    "        print(f\"\\nüìà TRIPLE DISTRIBUTION:\")\n",
    "        print(f\"   Min triples per chunk: {metrics_df['final_triple_count'].min()}\")\n",
    "        print(f\"   Max triples per chunk: {metrics_df['final_triple_count'].max()}\")\n",
    "        print(f\"   Median triples per chunk: {metrics_df['final_triple_count'].median()}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"   ‚ùå Could not find extraction results files\")\n",
    "    \n",
    "    # 7. Sample chunk analysis\n",
    "    print(f\"\\nüîç SAMPLE CHUNKS:\")\n",
    "    for i in [0, len(chunks)//2, -1]:  # First, middle, last\n",
    "        chunk_num = i if i >= 0 else len(chunks) + i\n",
    "        print(f\"   Chunk {chunk_num+1}: {len(chunks[i])} chars\")\n",
    "        print(f\"   Preview: {chunks[i][:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    return {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"coverage_ratio\": coverage_ratio,\n",
    "        \"avg_chunk_length\": sum(chunk_lengths)/len(chunk_lengths),\n",
    "        \"chunks_match\": len(chunks) == 49\n",
    "    }\n",
    "\n",
    "# üöÄ RUN THE VERIFICATION\n",
    "if __name__ == \"__main__\":\n",
    "    results = verify_chunk_coverage()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    if results[\"chunks_match\"] and results[\"coverage_ratio\"] > 0.95:\n",
    "        print(\"‚úÖ VERDICT: Your 49 chunks look GOOD!\")\n",
    "        print(\"   - Chunk count matches\")\n",
    "        print(\"   - High content coverage\")\n",
    "        print(\"   - Reasonable chunk sizes\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è VERDICT: Consider adjusting chunk settings\")\n",
    "        if not results[\"chunks_match\"]:\n",
    "            print(\"   - Chunk count mismatch detected\")\n",
    "        if results[\"coverage_ratio\"] <= 0.95:\n",
    "            print(\"   - Potential content loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1183b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
